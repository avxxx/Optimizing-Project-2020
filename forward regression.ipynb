{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets, linear_model\n",
    "import scipy.linalg as sla\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import Counter\n",
    "from IPython.core.display import display, HTML\n",
    "sns.set_style('darkgrid')\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix $A \\in \\mathbb{R}^{n\\times m}$, n -- number of samples, m -- number of features\\\\\n",
    "\n",
    "Vector $y\\in \\mathbb{R}^n$\n",
    "\n",
    "We suppose thar $A$ and $y$ are normalized to 1\n",
    "\n",
    "Covariance matrix $C \\in \\mathbb{R}^{d\\times d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "m = 20\n",
    "A = np.random.rand(m, n)\n",
    "y = np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 100)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function below returns a covariance matrix C (m x m) for columns x_1, x_2,... of matrix X (m x n)\n",
    "# normalized to have expectation 0 and variation 1\n",
    "def get_covariance_matrix(X):\n",
    "    cov_matrix = np.cov(X, bias=True)\n",
    "    return cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns normalized vector of covariances between y and columns x_1, x_2,...of matrix X \n",
    "# \n",
    "def get_covariance_vector(X, y):\n",
    "    return np.cov(np.vstack((X_T, y)))[-1,:-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y_true, y_pred, *, sample_weight=None,\n",
    "             multioutput=\"uniform_average\"):\n",
    "    \"\"\"R^2 (coefficient of determination) regression score function.\n",
    "    Best possible score is 1.0 and it can be negative (because the\n",
    "    model can be arbitrarily worse). A constant model that always\n",
    "    predicts the expected value of y, disregarding the input features,\n",
    "    would get a R^2 score of 0.0.\n",
    "    Read more in the :ref:`User Guide <r2_score>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "    sample_weight : array-like of shape (n_samples,), optional\n",
    "        Sample weights.\n",
    "    multioutput : string in ['raw_values', 'uniform_average', \\\n",
    "'variance_weighted'] or None or array-like of shape (n_outputs)\n",
    "        Defines aggregating of multiple output scores.\n",
    "        Array-like value defines weights used to average scores.\n",
    "        Default is \"uniform_average\".\n",
    "        'raw_values' :\n",
    "            Returns a full set of scores in case of multioutput input.\n",
    "        'uniform_average' :\n",
    "            Scores of all outputs are averaged with uniform weight.\n",
    "        'variance_weighted' :\n",
    "            Scores of all outputs are averaged, weighted by the variances\n",
    "            of each individual output.\n",
    "        .. versionchanged:: 0.19\n",
    "            Default value of multioutput is 'uniform_average'.\n",
    "    Returns\n",
    "    -------\n",
    "    z : float or ndarray of floats\n",
    "        The R^2 score or ndarray of scores if 'multioutput' is\n",
    "        'raw_values'.\n",
    "    Notes\n",
    "    -----\n",
    "    This is not a symmetric function.\n",
    "    Unlike most other scores, R^2 score may be negative (it need not actually\n",
    "    be the square of a quantity R).\n",
    "    This metric is not well-defined for single samples and will return a NaN\n",
    "    value if n_samples is less than two.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] `Wikipedia entry on the Coefficient of determination\n",
    "            <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.metrics import r2_score\n",
    "    >>> y_true = [3, -0.5, 2, 7]\n",
    "    >>> y_pred = [2.5, 0.0, 2, 8]\n",
    "    >>> r2_score(y_true, y_pred)\n",
    "    0.948...\n",
    "    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "    >>> r2_score(y_true, y_pred,\n",
    "    ...          multioutput='variance_weighted')\n",
    "    0.938...\n",
    "    >>> y_true = [1, 2, 3]\n",
    "    >>> y_pred = [1, 2, 3]\n",
    "    >>> r2_score(y_true, y_pred)\n",
    "    1.0\n",
    "    >>> y_true = [1, 2, 3]\n",
    "    >>> y_pred = [2, 2, 2]\n",
    "    >>> r2_score(y_true, y_pred)\n",
    "    0.0\n",
    "    >>> y_true = [1, 2, 3]\n",
    "    >>> y_pred = [3, 2, 1]\n",
    "    >>> r2_score(y_true, y_pred)\n",
    "    -3.0\n",
    "    \"\"\"\n",
    "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
    "        y_true, y_pred, multioutput)\n",
    "    check_consistent_length(y_true, y_pred, sample_weight)\n",
    "\n",
    "    if _num_samples(y_pred) < 2:\n",
    "        msg = \"R^2 score is not well-defined with less than two samples.\"\n",
    "        warnings.warn(msg, UndefinedMetricWarning)\n",
    "        return float('nan')\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = column_or_1d(sample_weight)\n",
    "        weight = sample_weight[:, np.newaxis]\n",
    "    else:\n",
    "        weight = 1.\n",
    "\n",
    "    numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,\n",
    "                                                      dtype=np.float64)\n",
    "    denominator = (weight * (y_true - np.average(\n",
    "        y_true, axis=0, weights=sample_weight)) ** 2).sum(axis=0,\n",
    "                                                          dtype=np.float64)\n",
    "    nonzero_denominator = denominator != 0\n",
    "    nonzero_numerator = numerator != 0\n",
    "    valid_score = nonzero_denominator & nonzero_numerator\n",
    "    output_scores = np.ones([y_true.shape[1]])\n",
    "    output_scores[valid_score] = 1 - (numerator[valid_score] /\n",
    "                                      denominator[valid_score])\n",
    "    # arbitrary set to zero to avoid -inf scores, having a constant\n",
    "    # y_true is not interesting for scoring a regression anyway\n",
    "    output_scores[nonzero_numerator & ~nonzero_denominator] = 0.\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            # return scores individually\n",
    "            return output_scores\n",
    "        elif multioutput == 'uniform_average':\n",
    "            # passing None as weights results is uniform mean\n",
    "            avg_weights = None\n",
    "        elif multioutput == 'variance_weighted':\n",
    "            avg_weights = denominator\n",
    "            # avoid fail on constant y or one-element arrays\n",
    "            if not np.any(nonzero_denominator):\n",
    "                if not np.any(nonzero_numerator):\n",
    "                    return 1.0\n",
    "                else:\n",
    "                    return 0.0\n",
    "    else:\n",
    "        avg_weights = multioutput\n",
    "\n",
    "    return np.average(output_scores, weights=avg_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n"
     ]
    }
   ],
   "source": [
    "C = get_covariance_matrix(A)\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "b = get_covariance_vector(A, y)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019258634477752175\n"
     ]
    }
   ],
   "source": [
    "# R-squared coefficient\n",
    "# \n",
    "R_2 = np.dot(b.T, np.dot(LA.inv(C), b))\n",
    "print(R_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19737756 0.75686821 0.87705782 0.07998631 0.81287681]\n",
      " [0.57081788 0.13171926 0.40505844 0.21287215 0.35543807]\n",
      " [0.25083931 0.68852299 0.33749733 0.15015977 0.10220112]]\n",
      "(3, 5)\n",
      "[0.84156902 0.08420507 0.77366252 0.87797207 0.7480465 ] \n",
      "y_true shape (5,)\n",
      "(3,)\n",
      "[0.69445541 1.19696823 1.22152442 0.30391545 0.97247455]\n",
      "[[ 0.1127975  -0.00790294  0.02571064]\n",
      " [-0.00790294  0.02343482 -0.01473869]\n",
      " [ 0.02571064 -0.01473869  0.04323573]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "n_test = 5\n",
    "m_test = 3\n",
    "A_test = np.random.rand(m_test, n_test)\n",
    "print(A_test)\n",
    "print(A_test.shape)\n",
    "y_test = np.random.rand(n_test)\n",
    "y_true = y_test\n",
    "print(y_true, '\\ny_true shape', y_true.shape)\n",
    "alpha = np.random.rand(m_test)\n",
    "print(alpha.shape)\n",
    "y_pred = np.dot(alpha, A_test)\n",
    "print(y_pred)\n",
    "print(get_covariance_matrix(A_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn r2 score is:  -1.613043819880836\n"
     ]
    }
   ],
   "source": [
    "# r_2 = (np.mean())/()\n",
    "\n",
    "print('sklearn r2 score is: ', r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество признаков\n",
    "n = 30\n",
    "# размер выборки\n",
    "m = 100\n",
    "def data_generator(n, m, cov=0.2):\n",
    "  # cov - коэффициент, задающий ковариацию между значениями\n",
    "  # исходная мартица ковариации (может не быть положительно определенной) \n",
    "  pred_C = (1 - cov) * np.eye(n) + cov * np.ones((n, n))\n",
    "  # вектор средних значений\n",
    "  mean = np.zeros((n, ))\n",
    "  # новая матрица ковариации, близкая к исходной, но положительно определенная\n",
    "  # и с более разнообразными собственными значениями\n",
    "  C = np.cov(sps.multivariate_normal(mean, pred_C).rvs(size=m).T)\n",
    "  # поделим строки и стобцы на корень из диагонали чтобы генерируемые Х имели\n",
    "  # единичную дисперсию\n",
    "  diag_sqrt = np.sqrt(np.diag(C))\n",
    "  C = C / diag_sqrt\n",
    "  C = C.T / diag_sqrt\n",
    "  # генерируем выборку - она имеет нулевое матожидание и (почти) единичную \n",
    "  # дисперсию в каждом столбце\n",
    "  X = sps.multivariate_normal(mean, C).rvs(size=m)\n",
    "  # генерируем коэффициенты и делим их на соответствующее число, чтобы в итоге\n",
    "  # дисперсия Z была равна 1\n",
    "  a = sps.uniform(loc=0, scale=1).rvs(n)\n",
    "  a /= (a.dot(a) * (1 - cov) + cov * (a.sum())**2)\n",
    "  # генерируем предсказываемую переменную так, чтобы ее дисперсия была равна 1\n",
    "  # для этого делим на соответствующий коэффициент \n",
    "  Z = X.dot(a) + sps.norm(loc=0, scale=0.1).rvs(size=m)\n",
    "  \n",
    "  return X, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20) (100,)\n"
     ]
    }
   ],
   "source": [
    "m=20\n",
    "n=100\n",
    "X, Z=data_generator(m, n, 0.2)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input:  X (m x n) - normalized feature matrix \n",
    "#         k - selection parameter \n",
    "#         y_true - predictor variable \n",
    "\n",
    "def forward_regression(X, k, y_true):\n",
    "    (m,n)=X.shape\n",
    "#     list of values x1,x2... indexes \n",
    "    V=range(m)\n",
    "    S=[]\n",
    "    used=[]\n",
    "    scores=[]\n",
    "    r2 = -2\n",
    "    alpha = np.zeros(m)\n",
    "    alpha_new = np.zeros(m)\n",
    "    j_max = 0\n",
    "    res_sum = 0\n",
    "    \n",
    "    for i in range(k):\n",
    "        \n",
    "        for j in V:\n",
    "            if j not in S:\n",
    "                \n",
    "                alpha_tmp=alpha\n",
    "                alpha_tmp[j]=1\n",
    "                \n",
    "                y_pred = np.dot(alpha_tmp, X)\n",
    "            \n",
    "                if r2_score(y_true, y_pred) > r2:\n",
    "                    r2 = r2_score(y_true, y_pred)\n",
    "                    alpha_new = alpha_tmp\n",
    "                    j_max=j\n",
    "                 \n",
    "                \n",
    "                \n",
    "                \n",
    "        alpha = alpha_new    \n",
    "        S.append(j_max)\n",
    "#         V.remove(j_max)\n",
    "        \n",
    "    return S, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_regression(X, k, y_true):\n",
    "    (m,n)=X.shape\n",
    "#     list of values x1,x2... indexes \n",
    "    V=range(m)\n",
    "    S=[]\n",
    "    used=[]\n",
    "    scores=[]\n",
    "    r2 = -2\n",
    "    alpha = np.zeros(m)\n",
    "    alpha_new = np.zeros(m)\n",
    "    j_max = 0\n",
    "    res_sum = np.zeros(n)\n",
    "    \n",
    "    for i in range(k):\n",
    "        \n",
    "        for j in V:\n",
    "            if j not in S:\n",
    "                y_pred = res_sum + X[j]\n",
    "            \n",
    "                if r2_score(y_true, y_pred) > r2:\n",
    "                    r2 = r2_score(y_true, y_pred)\n",
    "                    j_max=j\n",
    "                 \n",
    "        res_sum = res_sum + X[j_max]        \n",
    "                \n",
    "                \n",
    "#         alpha = alpha_new    \n",
    "        S.append(j_max)\n",
    "#         V.remove(j_max)\n",
    "        \n",
    "    return S, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0],\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]))"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape\n",
    "# y_true.shape\n",
    "forward_regression(A, 5, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = np.zeros(5)\n",
    "alpha[2] = 1\n",
    "alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
